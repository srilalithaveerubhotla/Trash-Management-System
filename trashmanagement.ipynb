{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trashmanagement.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilalithaveerubhotla/Trash-Management-System/blob/master/trashmanagement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu4nYpLnCZf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy\n",
        "import glob\n",
        "import pylab as plt\n",
        "import os\n",
        "\n",
        "# importing libraries\n",
        "from matplotlib import pyplot\n",
        "from keras import datasets\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,Flatten\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from datetime import datetime\n",
        "from packaging import version\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from keras.layers import Conv2D, Flatten, MaxPooling2D,Dense,Dropout,SpatialDropout2D\n",
        "from keras.models  import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\n",
        "import random,os,glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from kerastuner.tuners import RandomSearch\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "from keras.utils import print_summary, to_categorical\n",
        "\n",
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
        "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
        "from datetime import datetime\n",
        "%load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Vn26ogiJ8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOnuRYF2nENA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/garbage-classification.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsilLSTEiJ_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing and deriving the training images and labelling them\n",
        "trash_images = []\n",
        "labels = [] \n",
        "for fruit_dir_path in glob.glob(\"/content/garbage classification/Garbage classification/*/\"):\n",
        "    fruit_label = fruit_dir_path.split(\"/\")[4]\n",
        "    for image_path in glob.glob(os.path.join(fruit_dir_path, \"*.jpg\")):\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "        \n",
        "        image = cv2.resize(image, (70, 70))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "        \n",
        "        trash_images.append(image)\n",
        "        labels.append(fruit_label)\n",
        "trash_images = np.array(trash_images)\n",
        "labels = np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrL6cadI8pi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58zinSfGiKEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_to_id_dict = {v:i for i,v in enumerate(np.unique(labels))}\n",
        "id_to_label_dict = {v: k for k, v in label_to_id_dict.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUHQ-MzfiKGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_label_id = np.array([label_to_id_dict[i] for i in labels])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ffvgZ_NlIvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_to_label_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zgIpJlvlQwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Train_data shape......:\",trash_images.shape)\n",
        "print(\"Labels shape..........:\",training_label_id.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39KRzsDFlwWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(trainX, testX, trainY, testY) = train_test_split(trash_images, training_label_id, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi-vklch6tc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "now = datetime.now()\n",
        "logdir = \"logs/trash_images/\" + now.strftime(\"%Y%m%d-%H%M%S\")\n",
        "file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
        "with file_writer.as_default():\n",
        "  tf.summary.image(\"Training data\", np.reshape(trainX[0:10],(-2,70,70,3)), step=0)\n",
        "file_writer.set_as_default()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAr0S6hl7fBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/trash_images/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh4E3IZe9q20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir_path = '/content/garbage classification/Garbage classification'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZJCgLhj9wuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_list = glob.glob(os.path.join(dir_path, '*/*.jpg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJj0PMHB90EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(img_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jNetNoEQBwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# img_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvbXWwe6qLp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# img_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiOmJhAdbuUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i, img_path in enumerate(random.sample(img_list, 6)):\n",
        "#     img = image.load_img(img_path, target_size=(224, 224))\n",
        "#     img = image.img_to_array(img, dtype=np.uint8)\n",
        "#     plt.subplot(2, 3, i+1)\n",
        "#     plt.imshow(img.squeeze())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5268yDn9h_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train=ImageDataGenerator(horizontal_flip=True,\n",
        "                         vertical_flip=True,\n",
        "                         validation_split=0.1,\n",
        "                         rescale=1./255,\n",
        "                         shear_range = 0.1,\n",
        "                         zoom_range = 0.1,\n",
        "                         width_shift_range = 0.1,\n",
        "                         height_shift_range = 0.1,)\n",
        "\n",
        "test=ImageDataGenerator(rescale=1/255,\n",
        "                        validation_split=0.1)\n",
        "\n",
        "train_generator=train.flow_from_directory(dir_path,\n",
        "                                          target_size=(300,300),\n",
        "                                          batch_size=32,\n",
        "                                          class_mode='categorical',\n",
        "                                          subset='training')\n",
        "\n",
        "test_generator=test.flow_from_directory(dir_path,\n",
        "                                        target_size=(300,300),\n",
        "                                        batch_size=32,\n",
        "                                        class_mode='categorical',\n",
        "                                        subset='validation')\n",
        "\n",
        "validation_generator = test.flow_from_directory(\n",
        "    dir_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    seed=0\n",
        ")\n",
        "\n",
        "labels = (train_generator.class_indices)\n",
        "print(labels)\n",
        "\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "print(labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXE9QHzk-HkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for image_batch, label_batch in train_generator:\n",
        "  break\n",
        "image_batch.shape, label_batch.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Z0QT_D-LKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (train_generator.class_indices)\n",
        "\n",
        "Labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
        "\n",
        "with open('labels.txt', 'w') as f:\n",
        "  f.write(Labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dpyuvyOR_lt",
        "colab_type": "text"
      },
      "source": [
        "## Simple CNN Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58UtoGdp-NXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model=tf.keras.Sequential()\n",
        "#Convolution blocks\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(32,(3,3), padding='same',input_shape=(300,300,3),activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2)) \n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(64,(3,3), padding='same',activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2)) \n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(32,(3,3), padding='same',activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2)) \n",
        "\n",
        "#Classification layers\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(64,activation='relu'))\n",
        "#model.add(SpatialDropout2D(0.5))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(32,activation='relu'))\n",
        "\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(6,activation='softmax'))\n",
        "\n",
        "filepath=\"/content/trained_model.h5\"\n",
        "logdir=\"logs/trash_images/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"trash_simple_model.h5\", save_best_only=True)\n",
        "callbacks_list = [model_checkpoint,tensorboard_callback]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoRpItrF-RsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNftt_FGxqXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(model, \"simple_cnn_trash.png\", show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adnJdTFa-WHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc','mse']) # RMS PROP - No accuracy\n",
        "\n",
        "#es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbjEDQoX-Zlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              epochs=10,\n",
        "                              steps_per_epoch=2276//32,\n",
        "                              validation_data=test_generator,\n",
        "                              validation_steps=251//32,\n",
        "                              workers = 4,\n",
        "                              callbacks=callbacks_list) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkqW0nLU5EYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorboard dev upload --logdir logs/trash_images/ \\\n",
        "  --name \"Trash Management\" \\\n",
        "  --description \"Training results from https://colab.sandbox.google.com/github/tensorflow/tensorboard/blob/master/docs/tbdev_getting_started.ipynb\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8LDWsJdd78B",
        "colab_type": "text"
      },
      "source": [
        "## Simple CNN Fine Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO57c6gLeBQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=tf.keras.Sequential()\n",
        "#Convolution blocks\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(32,(3,3), padding='same',input_shape=(300,300,3),activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2)) \n",
        "model.add(tf.keras.layers.Dropout(0.5)) # No accuracy\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(64,(3,3), padding='same',activation='selu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2)) \n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(32,(3,3), padding='same',activation='selu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2)) \n",
        "\n",
        "#Classification layers\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(126,activation='selu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(64,activation='selu'))\n",
        "\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "\n",
        "model.add(tf.keras.layers.Dense(32,activation='selu'))\n",
        "#Classification layers\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(16,activation='selu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(6,activation='softmax'))\n",
        "\n",
        "filepath=\"/content/trained_model.h5\"\n",
        "logdir=\"logs/trash_images/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"trash_simple_model1.h5\", save_best_only=True)\n",
        "callbacks_list = [model_checkpoint,tensorboard_callback]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byI9r8tLd7Q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL9RjHCe-bw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(model, \"simple_cnn1_trash.png\", show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNjgc3cJfAvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='kullback_leibler_divergence', optimizer='nadam', metrics=['acc'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ8TJ2rq_Hug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              epochs=10,\n",
        "                              steps_per_epoch=2276//32,\n",
        "                              validation_data=test_generator,\n",
        "                              validation_steps=251//32,\n",
        "                              workers = 4,\n",
        "                              callbacks=callbacks_list) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "260gF6dn80ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p classifier1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_NTUenzBFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_save_name = 'classifier1.h5'\n",
        "model.save('/content/trained_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIVHc77rzqoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"/content/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpWbNMoSoYX7",
        "colab_type": "text"
      },
      "source": [
        "### VGG16 TRANSFER LEARNING Using Pretrained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWV_6F0foA7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.utils.vis_utils import plot_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzBlsFmvmCqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = VGG16()\n",
        "plot_model(model, to_file='vgg.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC6Hj4Y1ocKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = VGG16()\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2B6Mq-2pDFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "# load an image from file\n",
        "image = load_img('/content/garbage classification/Garbage classification/paper/paper357.jpg', target_size=(224, 224))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY1N2X5upk_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "# convert the image pixels to a numpy array\n",
        "image = img_to_array(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx9B1PJ2qgSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape data for the model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pyd6eH8qitp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import preprocess_input\n",
        "# prepare the image for the VGG model\n",
        "image = preprocess_input(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3t7cMt2qlv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yhat = model.predict(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwRh7qmfqpXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import decode_predictions\n",
        "# convert the probabilities to class labels\n",
        "label = decode_predictions(yhat)\n",
        "# retrieve the most likely result, e.g. highest probability\n",
        "label = label[0][0]\n",
        "# print the classification\n",
        "print('%s (%.2f%%)' % (label[1], label[2]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN8InFBIQHrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/keras-pretrained-models.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yWWGlO4q1FQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = {\n",
        "    \"train\": ImageDataGenerator(\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        rescale=1. / 255,\n",
        "        validation_split=0.1,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        rotation_range=30,\n",
        "    ).flow_from_directory(\n",
        "        directory=dir_path,\n",
        "        target_size=(300, 300),\n",
        "        subset='training',\n",
        "    ),\n",
        "\n",
        "    \"valid\": ImageDataGenerator(\n",
        "        rescale=1 / 255,\n",
        "        validation_split=0.1,\n",
        "    ).flow_from_directory(\n",
        "        directory=dir_path,\n",
        "        target_size=(300, 300),\n",
        "        subset='validation',\n",
        "    ),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57GIVpr_2UWy",
        "colab_type": "text"
      },
      "source": [
        "## VGG 16 From Scratch code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q3hucjPClFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui2LAaJTbN8_",
        "colab_type": "text"
      },
      "source": [
        "## INCEPTION V3 TOP Notch Transfer Learned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNlS8azOO_Ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers, losses\n",
        "from keras.layers import Conv2D, Flatten, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import pickle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8nHjcAqP_yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = InceptionV3(weights=None, include_top=False, input_shape=(300, 300, 3))\n",
        "base_model.load_weights('/content/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "base_model.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB9zgAD8P_11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dropout(0.15),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "opt = optimizers.nadam(lr=0.0001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "batch_size = 200\n",
        "epochs = 10\n",
        "train_generator = gen[\"train\"]\n",
        "valid_generator = gen[\"valid\"]\n",
        "\n",
        "steps_per_epoch = train_generator.n // batch_size\n",
        "validation_steps = valid_generator.n // batch_size\n",
        "\n",
        "filepath = \"model_{epoch:02d}-{val_accuracy:.2f}.h5\"\n",
        "checkpoint1 = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint1]\n",
        "history = model.fit_generator(generator=train_generator, epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
        "                              validation_data=valid_generator, validation_steps=validation_steps,\n",
        "                              callbacks=callbacks_list)\n",
        "with open('trainHistoryDict.txt', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fZO0Cc7aYTX",
        "colab_type": "text"
      },
      "source": [
        "## RESNET 50 top notch Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykL4g8R-QzMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers, losses\n",
        "from keras.layers import Conv2D, Flatten, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import pickle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXeg8Vkyaq2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = VGG16(weights=None, include_top=False, input_shape=(300, 300, 3))\n",
        "base_model.load_weights('/content/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "base_model.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0dffqKxa6_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dropout(0.15),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "opt = optimizers.nadam(lr=0.0001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "batch_size = 200\n",
        "epochs = 10\n",
        "train_generator = gen[\"train\"]\n",
        "valid_generator = gen[\"valid\"]\n",
        "\n",
        "steps_per_epoch = train_generator.n // batch_size\n",
        "validation_steps = valid_generator.n // batch_size\n",
        "\n",
        "filepath = \"model_{epoch:02d}-{val_accuracy:.2f}.h5\"\n",
        "checkpoint1 = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint1]\n",
        "history = model.fit_generator(generator=train_generator, epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
        "                              validation_data=valid_generator, validation_steps=validation_steps,\n",
        "                              callbacks=callbacks_list)\n",
        "with open('trainHistoryDict.txt', 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeSWENrVPLZZ",
        "colab_type": "text"
      },
      "source": [
        "## RESNET 50 SCRATCH CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYZSLvHjSY-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "#from resnets_utils import *\n",
        "from keras.initializers import glorot_uniform\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS-IoB8-RxT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(X, f, filters, stage, block):\n",
        "    \n",
        "    # Defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1, 1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path \n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpUxYb8jSJn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convolutional_block(X, f, filters, stage, block, s=2):\n",
        "\n",
        "    # Defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First component of main path \n",
        "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    ##### SHORTCUT PATH #### \n",
        "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6LgnxMVPR10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ResNet50(input_shape = (64, 64, 3), classes = 6):\n",
        "    \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    \n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    # Stage 4\n",
        "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    # Stage 5\n",
        "    X = convolutional_block(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL\n",
        "    X = AveragePooling2D(pool_size=(2,2), padding='same')(X)\n",
        "\n",
        "    # Output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH9LaRwITJjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize image vectors\n",
        "X_train = trainX/255.\n",
        "X_test = testX/255.\n",
        "\n",
        "# Convert training and test labels to one hot matrices\n",
        "Y_train = to_categorical(trainY, 6)\n",
        "Y_test = to_categorical(testY, 6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdk2EQ_ZUMnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
        "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
        "print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"Y_train shape: \" + str(Y_train.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"Y_test shape: \" + str(Y_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voKZnTYqPR5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ResNet50(input_shape = (70, 70, 3), classes = 6)\n",
        "keras.utils.plot_model(model, \"simple_RESNET_trash.png\", show_shapes=True)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a9fn8u5PR8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_train,Y_train, epochs = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKJZmpE6ceF4",
        "colab_type": "text"
      },
      "source": [
        "## MobileNet Transfer Learned Top Notch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTLODoUbbBPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SHAPE = (224,224,3)\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False, \n",
        "                                               weights='imagenet')\n",
        "base_model.trainable = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LShYTGqcrJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\n",
        "  tf.keras.layers.Dense(6, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ou3GxDScwyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001), #Adam(), \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2CDfFQ-cz97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 50\n",
        "steps_per_epoch = train_generator.n // batch_size\n",
        "validation_steps = validation_generator.n // batch_size\n",
        "\n",
        "history = model.fit_generator(train_generator, \n",
        "                              steps_per_epoch = steps_per_epoch,\n",
        "                              epochs=epochs, \n",
        "                              workers=4,\n",
        "                              validation_data=validation_generator, \n",
        "                              validation_steps=validation_steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7bTuBggFR6n",
        "colab_type": "text"
      },
      "source": [
        "## MOBILE NET From Scratch code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrxbpkOMDmVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_X = np.array( trainX ) / 255\n",
        "# train_Y = np.array( trainY )\n",
        "# test_X = np.array( testX ) / 255\n",
        "# test_Y = np.array( testY )\n",
        "\n",
        "# train_Y = tf.keras.utils.to_categorical( train_Y , num_classes=6 )\n",
        "# test_Y = tf.keras.utils.to_categorical( test_Y , num_classes=6 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfVfpll6IG1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV6RMVrLGNIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Note: You may use tf.keras.layers.DepthwiseConv2D but you won't be able to add BatchNorm and LeakyReLU layers.\n",
        "# Hence, we are first performing depthwise convolutions and then a Conv2D with kernel size of 1.\n",
        "def SeparableConv( x , num_filters , strides , alpha=1.0 ):\n",
        "    x = tf.keras.layers.DepthwiseConv2D( kernel_size=3 , padding='same' )( x )\n",
        "    x = tf.keras.layers.BatchNormalization(momentum=0.9997)( x )\n",
        "    x = tf.keras.layers.Activation( 'relu' )( x )\n",
        "    x = tf.keras.layers.Conv2D( np.floor( num_filters * alpha ) , kernel_size=( 1 , 1 ) , strides=strides , use_bias=False , padding='same' )( x )\n",
        "    x = tf.keras.layers.BatchNormalization(momentum=0.9997)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def Conv( x , num_filters , kernel_size , strides=1 , alpha=1.0 ):\n",
        "    x = tf.keras.layers.Conv2D( np.floor( num_filters * alpha ) , kernel_size=kernel_size , strides=strides , use_bias=False , padding='same' )( x )\n",
        "    x = tf.keras.layers.BatchNormalization( momentum=0.9997 )(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "# The number of classes are three.\n",
        "num_classes = 6\n",
        "\n",
        "# The shape of the input image.\n",
        "inputs = tf.keras.layers.Input( shape=( 70 , 70 , 3))\n",
        "\n",
        "x = Conv( inputs , num_filters=32 , kernel_size=3 , strides=2 )\n",
        "x = SeparableConv( x , num_filters=32 , strides=1 )\n",
        "x = Conv( x , num_filters=64 , kernel_size=1 )\n",
        "x = SeparableConv( x , num_filters=64 , strides=2  )\n",
        "x = Conv( x , num_filters=128 , kernel_size=1 )\n",
        "x = SeparableConv( x , num_filters=128 , strides=1  )\n",
        "x = Conv( x , num_filters=128 , kernel_size=1 )\n",
        "x = SeparableConv( x , num_filters=128 , strides=2  )\n",
        "x = Conv( x , num_filters=256 , kernel_size=1 )\n",
        "x = SeparableConv( x , num_filters=256 , strides=1  )\n",
        "x = Conv( x , num_filters=256 , kernel_size=1 )\n",
        "x = SeparableConv( x , num_filters=256 , strides=2  )\n",
        "x = Conv( x , num_filters=512 , kernel_size=1 )\n",
        "\n",
        "# You may uncomment the code below if you're machine could tolerate such heavy computation!\n",
        "#for i in range( 5 ):\n",
        "    #x = SeparableConv(x, num_filters=512 , strides=1 )\n",
        "    #x = Conv(x, num_filters=512 , kernel_size=1 )\n",
        "\n",
        "x = SeparableConv(x, num_filters=512 , strides=2 )\n",
        "x = Conv(x, num_filters=1024 , kernel_size=1 )\n",
        "x = tf.keras.layers.AveragePooling2D( pool_size=( 7 , 7 ))( x )\n",
        "x = tf.keras.layers.Flatten()( x )\n",
        "x = tf.keras.layers.Dense( num_classes )( x )\n",
        "outputs = tf.keras.layers.Activation( 'softmax' )( x )\n",
        "\n",
        "model = tf.keras.models.Model( inputs , outputs )\n",
        "\n",
        "# As we doing classification, we'll use categorical crossentropy and the RMSProp optimizier.\n",
        "model.compile( loss='categorical_crossentropy' , optimizer='rmsprop' , metrics=[ 'acc' ] )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5V6jfSCHEQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(model, \"simple_cnn1_trash.png\", show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ego8KCpqGxzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnL1anLObGns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG4JqeylGNN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.fit(trainX, trainY, epochs=5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-ylEmWJGNS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCtXOA42GNX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ij8nbYiGNay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hZEIeFuGNd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1BdT6R3GNV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HuJp7KUGNQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkBQfHr4GNLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni2p9o2Vc2qT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # !git clone https://github.com/pjreddie/darknet\n",
        "# !cd darknet/\n",
        "# !make\n",
        "# !wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ChXm8L5lDJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}